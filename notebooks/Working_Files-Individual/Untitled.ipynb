{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA ENGINEERING PLATFORMS (MSCA 31012)\n",
    "Webscraping using Python ( Example 1 )\n",
    "References: https://first-web-scraper.readthedocs.io/en/latest/ http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html http://altitudelabs.com/blog/web-scraping-with-python-and-beautiful-soup/\n",
    "\n",
    "Installation: pip install BS4 | pip install Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BS4 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (from BS4) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (from beautifulsoup4->BS4) (1.8)\n",
      "Requirement already satisfied: Requests in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (from Requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (from Requests) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (from Requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cgarc\\anaconda3\\lib\\site-packages (from Requests) (2019.6.16)\n"
     ]
    }
   ],
   "source": [
    "!pip install BS4 \n",
    "!pip install Requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Rules\n",
    "You should check a websiteâ€™s Terms and Conditions before you scrape it. Be careful to read the statements about legal use of data. Usually, the data you scrape should not be used for commercial purposes.\n",
    "Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "The layout of a website may change from time to time, so make sure to revisit the site and rewrite your code as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the current Detainees of Boone County Jail from webpage into CSV\n",
    "url = 'http://proximityone.com/ziptractequiv.htm#table'\n",
    "headers = {'User-Agent': \"Chrome/54.0.2840.90\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "html = response.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML('<iframe src=https://msdh.ms.gov/msdhsite/_static/14,0,420.html width=900 height=350><iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")  #Feed the HTML page to BeautifulSoup. \n",
    "table = soup.find('span', id=\"aw36-rows-start\", attrs={'class': \"aw-view-top\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-73b36d2aab1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtmpRow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtmpRow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#BeautifulSoup gets us going by allowing us to dig down into our table and return a list of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#rows, which are created in HTML using <tr> tags inside the table.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "tmpRow = (table.findAll('span')[1:])  \n",
    "print (tmpRow)\n",
    "#BeautifulSoup gets us going by allowing us to dig down into our table and return a list of \n",
    "#rows, which are created in HTML using <tr> tags inside the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Editing the code to fetch details of every record. The 'Case Number' and 'Charge Description' is Fetched for every Record.\n",
    "list_of_rows = []\n",
    "try:\n",
    "    outfile = open(\"./PublicSchoolZip.csv\", \"w\")\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"ZIPCode\", \"StAb\", \"StCtyTract\", \"CtyTract\", \"TractCode1\", \"TractCode2\", \"Population\", \"Housing Units\"]) #instead of digging through headers, skip first row when and write headers here. \n",
    "    for row in table.findAll('span'):     #return list of rows which are created in HTML using <tr>\n",
    "        list_of_cells = []\n",
    "        for cell in row.findAll(\"span\"):  #Loop through cells in each row. Cells are created in HTML by <td>\n",
    "            #if(cell['align'] == ''):\n",
    "            #    continue\n",
    "            text = cell.text.replace('&nbsp;', '') #&nbsp; is html code for a non-breaking space which forces browser to render empty space. Delete it. \n",
    "            list_of_cells.append(text) #need CSV file (grid of columns and rows) do this by adding each cell in row to new Python list. \n",
    "        for anchor in row.findAll('th'):\n",
    "            #href is fetched for every record and the response is parsed for every individual hit to href.\n",
    "            details_scope = \"http://proximityone.com/ziptractequiv.htm#table\"+anchor['scope']\n",
    "            details_response = requests.get(details_scope)\n",
    "            details_html = details_response.content\n",
    "            details_soup = BeautifulSoup(details_html, \"html.parser\")\n",
    "            details_table = details_soup.find('table', attrs={'class': 'aw-templates-list aw-grid-view'})\n",
    "            if details_table is not None:\n",
    "                details_table_tr = details_table.find('span', attrs={\"class\": \"aw-hpanel-bottom \"})\n",
    "                if details_table_tr is not None:\n",
    "                    list_of_cells.append((details_table_tr.find('td', attrs = {'align' : 'CASE #'})).text) #lump lists into one big list of lists\n",
    "                    list_of_cells.append((details_table_tr.find('td', attrs = {'alig' : 'CHARGE DESCRIPTION'})).text)\n",
    "        arrLength = len(list_of_cells)\n",
    "        writer.writerow(list_of_cells) #dump out list of lists. \n",
    "finally:\n",
    "    outfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./PublicSchoolZip.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final results\n",
    "df.to_csv(\"./publicschoolZip.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
